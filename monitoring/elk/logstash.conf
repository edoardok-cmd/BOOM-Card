input {
  # Application logs from file
  file {
    path => "/var/log/boom-card/*.log"
    type => "application"
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"
      negate => true
      what => "previous"
    }
    tags => ["boom-card", "application"]
  }

  # Nginx access logs
  file {
    path => "/var/log/nginx/access.log"
    type => "nginx-access"
    tags => ["boom-card", "nginx", "access"]
  }

  # Nginx error logs
  file {
    path => "/var/log/nginx/error.log"
    type => "nginx-error"
    tags => ["boom-card", "nginx", "error"]
  }

  # PostgreSQL logs
  file {
    path => "/var/log/postgresql/*.log"
    type => "postgresql"
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"
      negate => true
      what => "previous"
    }
    tags => ["boom-card", "postgresql", "database"]
  }

  # Redis logs
  file {
    path => "/var/log/redis/*.log"
    type => "redis"
    tags => ["boom-card", "redis", "cache"]
  }

  # Docker container logs
  docker {
    type => "docker"
    tags => ["boom-card", "docker", "container"]
  }

  # Application metrics from StatsD
  udp {
    port => 8125
    type => "metrics"
    codec => plain
    tags => ["boom-card", "metrics", "statsd"]
  }

  # Beats input for additional sources
  beats {
    port => 5044
    type => "beats"
    tags => ["boom-card", "beats"]
  }

  # Syslog input for system logs
  syslog {
    port => 5514
    type => "syslog"
    tags => ["boom-card", "system"]
  }
}

filter {
  # Parse application logs
  if [type] == "application" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] \[%{DATA:service}\] \[%{DATA:correlationId}\] %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse JSON logs if present
    if [log_message] =~ /^\{.*\}$/ {
      json {
        source => "log_message"
        target => "parsed_json"
      }
    }

    # Extract user and partner information
    if [parsed_json][userId] {
      mutate {
        add_field => { "user_id" => "%{[parsed_json][userId]}" }
      }
    }
    
    if [parsed_json][partnerId] {
      mutate {
        add_field => { "partner_id" => "%{[parsed_json][partnerId]}" }
      }
    }

    # Extract transaction details
    if [parsed_json][transactionId] {
      mutate {
        add_field => { 
          "transaction_id" => "%{[parsed_json][transactionId]}"
          "transaction_amount" => "%{[parsed_json][amount]}"
          "discount_applied" => "%{[parsed_json][discountAmount]}"
        }
      }
    }

    # Extract API endpoint information
    if [parsed_json][endpoint] {
      mutate {
        add_field => { 
          "api_endpoint" => "%{[parsed_json][endpoint]}"
          "api_method" => "%{[parsed_json][method]}"
          "api_status" => "%{[parsed_json][statusCode]}"
          "api_duration" => "%{[parsed_json][duration]}"
        }
      }
    }
  }

  # Parse Nginx access logs
  if [type] == "nginx-access" {
    grok {
      match => {
        "message" => '%{IPORHOST:remote_ip} - %{DATA:user_name} \[%{HTTPDATE:time_local}\] "%{WORD:request_method} %{DATA:request_path} HTTP/%{NUMBER:http_version}" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} "%{DATA:http_referer}" "%{DATA:http_user_agent}" "%{DATA:request_time}"'
      }
    }
    
    # Convert numeric fields
    mutate {
      convert => {
        "response_code" => "integer"
        "body_sent_bytes" => "integer"
        "request_time" => "float"
      }
    }

    # Extract API version and endpoint
    if [request_path] =~ /^\/api\/v\d+\// {
      grok {
        match => {
          "request_path" => "/api/v%{INT:api_version}/%{GREEDYDATA:api_endpoint_path}"
        }
        tag_on_failure => []
      }
    }

    # GeoIP enrichment
    geoip {
      source => "remote_ip"
      target => "geoip"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}" ]
    }
  }

  # Parse PostgreSQL logs
  if [type] == "postgresql" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{DATA:timezone} \[%{NUMBER:pid}\] %{DATA:user}@%{DATA:database} %{LOGLEVEL:level}: %{GREEDYDATA:query_message}"
      }
    }

    # Extract slow query information
    if [query_message] =~ /duration:/ {
      grok {
        match => {
          "query_message" => "duration: %{NUMBER:query_duration:float} ms  statement: %{GREEDYDATA:sql_query}"
        }
        tag_on_failure => []
      }
      
      # Flag slow queries
      if [query_duration] > 1000 {
        mutate {
          add_tag => ["slow_query"]
          add_field => { "alert_type" => "slow_database_query" }
        }
      }
    }
  }

  # Parse Redis logs
  if [type] == "redis" {
    grok {
      match => {
        "message" => "%{NUMBER:pid}:%{DATA:role} %{GREEDYDATA:redis_message}"
      }
    }
  }

  # Parse metrics from StatsD
  if [type] == "metrics" {
    grok {
      match => {
        "message" => "%{DATA:metric_name}:%{NUMBER:metric_value:float}\|%{DATA:metric_type}(\|@%{NUMBER:sample_rate:float})?"
      }
    }

    # Parse metric namespace
    if [metric_name] =~ /^boom_card\./ {
      mutate {
        gsub => ["metric_name", "^boom_card\.", ""]
      }
      
      # Extract metric components
      ruby {
        code => "
          parts = event.get('metric_name').split('.')
          if parts.length >= 2
            event.set('metric_category', parts[0])
            event.set('metric_subcategory', parts[1])
            event.set('metric_detail', parts[2..-1].join('.')) if parts.length > 2
          end
        "
      }
    }
  }

  # Common enrichments for all logs
  mutate {
    add_field => { 
      "environment" => "${ENVIRONMENT:production}"
      "application" => "boom-card"
      "indexed_at" => "%{@timestamp}"
    }
  }

  # Parse user agent for web requests
  if [http_user_agent] {
    useragent {
      source => "http_user_agent"
      target => "user_agent"
    }
  }

  # Add response time categorization
  if [api_duration] {
    if [api_duration] <= 100 {
      mutate { add_field => { "performance_category" => "fast" } }
    } else if [api_duration] <= 500 {
      mutate { add_field => { "performance_category" => "normal" } }
    } else if [api_duration] <= 1000 {
      mutate { add_field => { "performance_category" => "slow" } }
    } else {
      mutate { add_field => { "performance_category" => "very_slow" } }
    }
  }

  # Security event detection
  if [response_code] {
    if [response_code] == 401 or [response_code] == 403 {
      mutate {
        add_tag => ["security_event", "authentication_failure"]
      }
    }
    
    if [response_code] == 429 {
      mutate {
        add_tag => ["security_event", "rate_limit_exceeded"]
      }
    }
  }

  # Transaction monitoring
  if [transaction_id] {
    mutate {
      add_tag => ["transaction"]
    }
    
    # Calculate discount percentage
    if [transaction_amount] and [discount_applied] {
      ruby {
        code => "
          amount = event.get('transaction_amount').to_f
          discount = event.get('discount_applied').to_f
          if amount > 0
            percentage = (discount / amount * 100).round(2)
            event.set('discount_percentage', percentage)
          end
        "
      }
    }
  }

  # Error detection and categorization
  if [level] == "ERROR" or [level] == "FATAL" {
    mutate {
      add_tag => ["error_event"]
    }
    
    # Extract error type
    if [log_message] =~ /Exception|Error/ {
      grok {
        match => {
          "log_message" => "%{DATA:error_type}(Exception|Error)"
        }
        tag_on_failure => []
      }
    }
  }

  # Date parsing
  date {
    match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target => "@timestamp"
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [ "message", "host", "log_message", "query_message", "redis_message" ]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:localhost:9200}"]
    index => "boom-card-%{type}-%{+YYYY.MM.dd}"
    document_type => "_doc"
    template_name => "boom-card"
    template => "/etc/logstash/templates/boom-card-template.json"
    template_overwrite => true
    
    # Authentication
    user => "${ELASTICSEARCH_USER:elastic}"
    password => "${ELASTICSEARCH_PASSWORD}"
    
    # SSL configuration
    ssl => true
    ssl_certificate_verification => true
    cacert => "/etc/logstash/certs/ca.crt"
  }

  # Alert critical errors to monitoring system
  if "error_event" in [tags] and [level] == "FATAL" {
    http {
      url => "${ALERT_WEBHOOK_URL}"
      http_method => "post"
      format => "json"
      mapping => {
        "service" => "%{service}"
        "level" => "%{level}"
        "error_type" => "%{error_type}"
        "timestamp" => "%{@timestamp}"
        "environment" => "%{environment}"
        "correlation_id" => "%{correlationId}"
      }
    }
  }

  # Output metrics to InfluxDB for Grafana
  if [type] == "metrics" {
    influxdb {
      host => "${INFLUXDB_HOST:localhost}"
      port => "${INFLUXDB_PORT:8086}"
      db => "boom_card_metrics"
      measurement => "%{metric_category}"
      tags => {
        "environment" => "%{environment}"
        "metric_type" => "%{metric_type}"
        "subcategory" => "%{metric_subcategory}"
      }
      fields => {
        "value" => "%{metric_value}"
      }
      use_event_fields_for_data_points => true
    }
  }

  # Debug output (disable in production)
  if "${DEBUG_MODE:false}" == "true" {
    stdout {
      codec => rubydebug
    }
  }
}
